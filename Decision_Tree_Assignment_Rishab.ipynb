{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1165b81",
   "metadata": {},
   "source": [
    "### 1. What is a Decision Tree, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089fe6c",
   "metadata": {},
   "source": [
    "A Decision Tree is a machine learning model that splits data into smaller parts using conditions, like a flowchart. Each internal node checks a condition on a feature, branches represent the outcome, and leaf nodes give the final prediction. It works by selecting the best feature at each step to split the data to get the most accurate result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66347e",
   "metadata": {},
   "source": [
    "### 2. What are impurity measures in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad448152",
   "metadata": {},
   "source": [
    "Impurity measures are used to check how mixed the classes are in a dataset. They help decide which feature to split on. The more pure the data (less mixed), the better the split. Common impurity measures are Gini Impurity and Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a7c60c",
   "metadata": {},
   "source": [
    "### 3. What is the mathematical formula for Gini Impurity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dee10",
   "metadata": {},
   "source": [
    "**Gini = 1 - Σ (pi²)**\n",
    "Where `pi` is the probability of class `i` in the node. It measures how often a randomly chosen element would be incorrectly labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f06b7d",
   "metadata": {},
   "source": [
    "### 4. What is the mathematical formula for Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1dc04",
   "metadata": {},
   "source": [
    "**Entropy = - Σ (pi * log₂(pi))**\n",
    "Where `pi` is the probability of class `i`. It measures the amount of uncertainty or randomness in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9593f98",
   "metadata": {},
   "source": [
    "### 5. What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ef881",
   "metadata": {},
   "source": [
    "Information Gain tells us how much 'information' a feature gives about the target variable. It is the reduction in entropy after a dataset is split. Decision Trees use it to pick the feature that reduces uncertainty the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc35f37",
   "metadata": {},
   "source": [
    "### 6. What is the difference between Gini Impurity and Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5290b2a",
   "metadata": {},
   "source": [
    "Both measure impurity, but Gini is simpler and faster to calculate. Entropy comes from information theory and includes logarithms. In practice, both give similar results, but Gini is used more often because of its speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a0142",
   "metadata": {},
   "source": [
    "### 7. What is the mathematical explanation behind Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3064b9",
   "metadata": {},
   "source": [
    "Mathematically, Decision Trees split the data at each node based on a condition that maximizes a metric (like Information Gain or Gini reduction). The algorithm keeps doing this until it reaches pure nodes or hits stopping conditions like max depth or min samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccea202",
   "metadata": {},
   "source": [
    "### 8. What is Pre-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662e7fa",
   "metadata": {},
   "source": [
    "Pre-Pruning means stopping the tree from growing too big during the building process. It uses conditions like max depth, min samples per node, etc., to avoid overfitting early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a77515",
   "metadata": {},
   "source": [
    "### 9. What is Post-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b57c5",
   "metadata": {},
   "source": [
    "Post-Pruning means first growing a full tree and then cutting back (removing) unnecessary branches after checking performance. It helps improve generalization by reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b11ff8b",
   "metadata": {},
   "source": [
    "### 10. What is the difference between Pre-Pruning and Post-Pruning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9b7a6",
   "metadata": {},
   "source": [
    "Pre-Pruning stops the tree from growing too much while it's being built. Post-Pruning cuts back the tree after it's fully grown. Both aim to reduce overfitting, but they work at different stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474066c",
   "metadata": {},
   "source": [
    "### 11. What is a Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb32e2",
   "metadata": {},
   "source": [
    "A Decision Tree Regressor is a type of Decision Tree used for predicting continuous values instead of classes. Instead of using classification rules, it calculates the average of values in each leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b594b5",
   "metadata": {},
   "source": [
    "### 12. What are the advantages and disadvantages of Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc341e",
   "metadata": {},
   "source": [
    "**Advantages:** Easy to understand, interpret, and visualize. Works well without much data prep.\n",
    "**Disadvantages:** Can overfit, especially with deep trees. Sensitive to small changes in data. Not good at capturing smooth patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655defa4",
   "metadata": {},
   "source": [
    "### 13. How does a Decision Tree handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8286e46",
   "metadata": {},
   "source": [
    "Decision Trees can handle missing values by either skipping them during split or assigning them to the branch that gives the best gain. Some libraries like scikit-learn can also impute or use surrogate splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68902f3e",
   "metadata": {},
   "source": [
    "### 14. How does a Decision Tree handle categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4b054",
   "metadata": {},
   "source": [
    "Decision Trees can split categorical features by checking each category or grouping them in different ways. Most implementations automatically handle them by converting categories into conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80735a8",
   "metadata": {},
   "source": [
    "### 15. What are some real-world applications of Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766a88b",
   "metadata": {},
   "source": [
    "Decision Trees are used in areas like medical diagnosis, fraud detection, loan approval, customer segmentation, and even game development. They're popular in business because they’re easy to explain to non-tech people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09018311",
   "metadata": {},
   "source": [
    "### Q16. Train a Decision Tree Classifier on the Iris dataset and print the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2850cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d881f0",
   "metadata": {},
   "source": [
    "### Q17. Train a Decision Tree Classifier using Gini and print feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Feature Importances:\", model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e98d0",
   "metadata": {},
   "source": [
    "### Q18. Train a Decision Tree Classifier using Entropy and print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a38bb3",
   "metadata": {},
   "source": [
    "### Q19. Train a Decision Tree Regressor on housing dataset and evaluate with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe943a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3c93d",
   "metadata": {},
   "source": [
    "### Q20. Train a Decision Tree Classifier and visualize using graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "dot_data = export_graphviz(model, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"iris_tree\", view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9850af3",
   "metadata": {},
   "source": [
    "### Q21. Train a tree with max_depth=3 and compare with full tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026edbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(max_depth=3)\n",
    "model2 = DecisionTreeClassifier()\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "acc1 = accuracy_score(y_test, model1.predict(X_test))\n",
    "acc2 = accuracy_score(y_test, model2.predict(X_test))\n",
    "\n",
    "print(\"Max Depth 3 Accuracy:\", acc1)\n",
    "print(\"Full Tree Accuracy:\", acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce24755",
   "metadata": {},
   "source": [
    "### Q22. Use min_samples_split=5 and compare with default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_default = DecisionTreeClassifier()\n",
    "tree_custom = DecisionTreeClassifier(min_samples_split=5)\n",
    "\n",
    "tree_default.fit(X_train, y_train)\n",
    "tree_custom.fit(X_train, y_train)\n",
    "\n",
    "print(\"Default Accuracy:\", accuracy_score(y_test, tree_default.predict(X_test)))\n",
    "print(\"min_samples_split=5 Accuracy:\", accuracy_score(y_test, tree_custom.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550bd88",
   "metadata": {},
   "source": [
    "### Q23. Apply feature scaling and compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_scaled, y)\n",
    "\n",
    "clf1 = DecisionTreeClassifier().fit(X_train1, y_train1)\n",
    "clf2 = DecisionTreeClassifier().fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Without Scaling Accuracy:\", accuracy_score(y_test1, clf1.predict(X_test1)))\n",
    "print(\"With Scaling Accuracy:\", accuracy_score(y_test2, clf2.predict(X_test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a03cd7",
   "metadata": {},
   "source": [
    "### Q24. Train Decision Tree Classifier using One-vs-Rest (OvR) strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d28cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "ovr_model = OneVsRestClassifier(DecisionTreeClassifier())\n",
    "ovr_model.fit(X_train, y_train)\n",
    "y_pred = ovr_model.predict(X_test)\n",
    "print(\"OvR Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c6b91",
   "metadata": {},
   "source": [
    "### Q25. Train Decision Tree Classifier and display feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e574a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "for name, score in zip(iris.feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61cc3d",
   "metadata": {},
   "source": [
    "### Q26. Train Decision Tree Regressor with max_depth=5 and compare with unrestricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = DecisionTreeRegressor(max_depth=5)\n",
    "reg2 = DecisionTreeRegressor()\n",
    "\n",
    "reg1.fit(X_train, y_train)\n",
    "reg2.fit(X_train, y_train)\n",
    "\n",
    "mse1 = mean_squared_error(y_test, reg1.predict(X_test))\n",
    "mse2 = mean_squared_error(y_test, reg2.predict(X_test))\n",
    "\n",
    "print(\"Depth 5 MSE:\", mse1)\n",
    "print(\"Unrestricted MSE:\", mse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2821b5",
   "metadata": {},
   "source": [
    "### Q27. Apply Cost Complexity Pruning (CCP) and visualize effect on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = DecisionTreeClassifier().cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "acc = []\n",
    "for alpha in ccp_alphas:\n",
    "    model = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ccp_alphas, acc)\n",
    "plt.xlabel(\"ccp_alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Effect of CCP on Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89481ed",
   "metadata": {},
   "source": [
    "### Q28. Evaluate performance using Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e05d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12f89f",
   "metadata": {},
   "source": [
    "### Q29. Visualize confusion matrix using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc72a2",
   "metadata": {},
   "source": [
    "### Q30. Use GridSearchCV to tune max_depth and min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}